{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_score(cm_score):\n",
    "    positive_or_zero_mask = cm_score >= 0\n",
    "    negative_mask = cm_score < 0\n",
    "    cm_score[positive_or_zero_mask] = 1\n",
    "    cm_score[negative_mask] = 0\n",
    "    return cm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 25/25 [00:23<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 493/493 [02:22<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: NWPU-RESISC45\n",
      "Spearman (Pseudo vs Real Accuracy): 0.48, Spearman (Real vs Real Accuracy): 0.89\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 59/59 [00:50<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 322/322 [03:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: Stanford_dogs\n",
      "Spearman (Pseudo vs Real Accuracy): 0.48, Spearman (Real vs Real Accuracy): 0.82\n",
      "Processed batch 1/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 98/98 [04:58<00:00,  3.05s/it]\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Wuy19\\AppData\\Local\\Temp\\ipykernel_100040\\3408598000.py:179: RuntimeWarning: Mean of empty slice.\n",
      "  cur_class_accuracy = (cur_class_pred == c).mean()\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 93, White breasted Nuthatch has NaN accuracy\n",
      "Processed batch 1/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 185/185 [02:33<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: CUB_200_2011\n",
      "Spearman (Pseudo vs Real Accuracy): nan, Spearman (Real vs Real Accuracy): 0.85\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 51/51 [01:11<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 11, black-eyed susan has NaN accuracy\n",
      "Class 26, colt's foot has NaN accuracy\n",
      "Class 32, desert-rose has NaN accuracy\n",
      "Class 44, globe-flower has NaN accuracy\n",
      "Class 47, hard-leaved pocket orchid has NaN accuracy\n",
      "Class 72, pink-yellow dahlia has NaN accuracy\n",
      "Class 79, ruby-lipped cattleya has NaN accuracy\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 128/128 [01:21<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: Flower102\n",
      "Spearman (Pseudo vs Real Accuracy): nan, Spearman (Real vs Real Accuracy): 0.79\n"
     ]
    }
   ],
   "source": [
    "# Image Method\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from utils.dataset import get_dataset\n",
    "from utils.model_utils import load_model, get_text_embeddings, get_image_embeddings\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "datasets_info = [\n",
    "    {\"name\": \"NWPU-RESISC45\", \"template\": \"a satellite photo containing {}.\"},\n",
    "    {\"name\": \"Stanford_dogs\", \"template\": \"a photo of {}, a type of dog.\"},\n",
    "    {\"name\": \"CUB_200_2011\", \"template\": \"a photo of {}, a type of bird.\"},\n",
    "    {\"name\": \"Flower102\", \"template\": \"a photo of {}, a type of flower.\"},\n",
    "]\n",
    "\n",
    "class PseudoImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, class_names, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Store class_names in a set for faster lookup\n",
    "        class_name_set = set(class_names)\n",
    "\n",
    "        for image_path in glob.glob(os.path.join(image_folder, '*.png')):\n",
    "            filename = os.path.basename(image_path)\n",
    "            class_name = filename.split('-')[0].replace('_', ' ')\n",
    "            if class_name in class_name_set:\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(class_name)\n",
    "\n",
    "        # Convert labels to indices\n",
    "        self.labels = [class_names.index(label) for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_image_loader(dataset, class_indices, batch_size, shuffle=False):\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        selected_indices = [i for i, label in enumerate(dataset.targets) if label in class_indices]\n",
    "    else:\n",
    "        selected_indices = [i for i, (_, label) in enumerate(dataset.imgs) if label in class_indices]\n",
    "    selected_dataset = Subset(dataset, selected_indices)\n",
    "\n",
    "    # Ensure no transform or compatible transform is applied\n",
    "    selected_dataset.dataset.transform = None\n",
    "\n",
    "    loader = DataLoader(\n",
    "        selected_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def pairwise_difference(a, b):\n",
    "  \"\"\"\n",
    "  Computes pairwise difference between rows of two matrices.\n",
    "\n",
    "  Args:\n",
    "    a: NumPy array of shape (N_a, D)\n",
    "    b: NumPy array of shape (N_b, D)\n",
    "\n",
    "  Returns:\n",
    "    NumPy array of shape (N_a, N_b, D) containing pairwise differences.\n",
    "  \"\"\"\n",
    "  return a[:, np.newaxis, :] - b[np.newaxis, :, :]\n",
    "\n",
    "def run_experiment(dataset_name, template, use_pseudo_data):\n",
    "    # 1. Load dataset (test split)\n",
    "    dataset = get_dataset(dataset_name, data_root='data', split='test')\n",
    "    class_names = dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    if use_pseudo_data:\n",
    "        PSEUDO_IMAGES_FOLDER = f'pseudo_images/{dataset_name}'\n",
    "        pseudo_dataset = PseudoImageDataset(\n",
    "            image_folder=PSEUDO_IMAGES_FOLDER,\n",
    "            class_names=class_names,\n",
    "            transform=None\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            pseudo_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "        )\n",
    "    else:\n",
    "        selected_class_indices = list(range(num_classes))\n",
    "        dataloader = get_image_loader(dataset, selected_class_indices, 64, shuffle=False)\n",
    "\n",
    "    # 2. Load model\n",
    "    model_info = load_model(model_name, device=device)\n",
    "\n",
    "    # 3. Create captions and compute text embeddings\n",
    "    captions = [template.format(c) for c in class_names]\n",
    "    text_embeddings = get_text_embeddings(captions, model_info, device, batch_size=64)\n",
    "\n",
    "    # 4. Compute image embeddings\n",
    "    image_embeddings, all_targets = get_image_embeddings(dataloader, model_info, device)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "    text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # img shape: (N_img, D), text shape: (N_text, D)\n",
    "\n",
    "    # 5. Compute similarities and predictions\n",
    "    temperature = 0.01\n",
    "    similarities = np.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "    probs = np.exp(similarities) / np.sum(np.exp(similarities), axis=1, keepdims=True)\n",
    "\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "\n",
    "    predicted_cm_scores = []\n",
    "    # do some class-wise operations\n",
    "    for c in range(num_classes):\n",
    "        # Select rows of image_embeddings where the target is class c\n",
    "        c_embeddings = image_embeddings[all_targets == c] # shape (N_c, D)\n",
    "\n",
    "        other_class_text_embeddings = np.concatenate([text_embeddings[i:i+1] for i in range(num_classes) if i != c], axis=0) # shape (N_t-1, D)\n",
    "        text_diff = pairwise_difference(text_embeddings[c:c+1], other_class_text_embeddings) # shape (1, N_t-1, D)\n",
    "        \n",
    "        c_embeddings = c_embeddings / np.linalg.norm(c_embeddings, axis=-1, keepdims=True)\n",
    "        text_diff = text_diff / np.linalg.norm(text_diff, axis=-1, keepdims=True)\n",
    "\n",
    "        cm_scores = np.einsum('cd, td -> ct', c_embeddings, text_diff[0]) # shape (N_c, N_t-1)\n",
    "        \n",
    "        # make softmax\n",
    "        def softmax(x):\n",
    "            \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "            e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        \n",
    "        def softmin(x):\n",
    "            \"\"\"Compute softmin values for each sets of scores in x (with numerical stability).\"\"\"\n",
    "            neg_x = -x\n",
    "            e_neg_x = np.exp(neg_x - np.max(neg_x, axis=-1, keepdims=True))\n",
    "            return e_neg_x / np.sum(e_neg_x, axis=-1, keepdims=True)\n",
    "        \n",
    "        cm_scores = softmax(1/cm_scores)\n",
    "\n",
    "        cm_score = 1 / np.max(cm_scores, axis=-1) # shape (N_c, 1)\n",
    "        # cm_score = cm_score / (1 + cm_score) # shape (N_c, 1)\n",
    "        cm_score = np.mean(cm_score, axis=0) # shape (1,)\n",
    "        predicted_cm_scores.append(cm_score)\n",
    "\n",
    "    # Compute class-wise actual and predicted accuracies\n",
    "    actual_accuracies = []\n",
    "    # predicted_accuracies = []\n",
    "    for c in range(num_classes):\n",
    "        cur_class_indices = (all_targets == c)\n",
    "        cur_class_pred = preds[cur_class_indices]\n",
    "        cur_class_accuracy = (cur_class_pred == c).mean()\n",
    "\n",
    "        if np.isnan(cur_class_accuracy):\n",
    "            print(f\"Class {c}, {class_names[c]} has NaN accuracy\")\n",
    "            cur_class_accuracy = 0.0\n",
    "        # predicted_cur_class_indices = (preds == c)\n",
    "        # predicted_confidences = confidences[predicted_cur_class_indices]\n",
    "        # predicted_acc = predicted_confidences.mean() if predicted_confidences.size > 0 else np.nan\n",
    "\n",
    "        actual_accuracies.append(cur_class_accuracy)\n",
    "        # predicted_accuracies.append(predicted_acc if not np.isnan(predicted_acc) else 0.0)\n",
    "            \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr = spearmanr(np.array(actual_accuracies), predicted_cm_scores )# np.array(predicted_accuracies))\n",
    "\n",
    "    # Return data for plotting outside\n",
    "    return np.array(actual_accuracies), np.array(predicted_cm_scores), spearman_corr\n",
    "\n",
    "# Now we call run_experiment for pseudo and real data and plot them together\n",
    "for dinfo in datasets_info:\n",
    "    actual_pseudo, predicted_pseudo, spearman_pseudo = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=True)\n",
    "    actual_real, predicted_real, spearman_real = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=False)\n",
    "\n",
    "    # Recompute pseudo Spearman correlation using actual_real for the y-values\n",
    "    pseudo_spearman_corr = spearmanr(actual_real, predicted_pseudo)\n",
    "\n",
    "    # Create a single plot for both pseudo and real\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    # For pseudo, use actual_real as y-axis\n",
    "    ax.scatter(predicted_pseudo, actual_real, color='tab:orange', label='Pseudo Images', alpha=0.5)\n",
    "    ax.scatter(predicted_real, actual_real, color='tab:blue', label='Real Images', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Accuracy (Confidence)\")\n",
    "    ax.set_ylabel(\"Actual Accuracy (Real Data)\")\n",
    "    pseudo_corr_str = f\"{pseudo_spearman_corr.correlation:.2f}\" if pseudo_spearman_corr.correlation is not None else \"N/A\"\n",
    "    real_corr_str = f\"{spearman_real.correlation:.2f}\" if spearman_real.correlation is not None else \"N/A\"\n",
    "    ax.set_title(f\"Calibration Approach: {dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    print(f\"Calibration Approach: {dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    ax.grid(True)\n",
    "    # ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plot_filename = f\"figures/{dinfo['name']}_CM_softmax.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 25/25 [00:23<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 493/493 [04:20<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: NWPU-RESISC45\n",
      "Spearman (Pseudo vs Real Accuracy): 0.22, Spearman (Real vs Real Accuracy): 0.40\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 59/59 [00:54<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 322/322 [03:19<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Approach: Stanford_dogs\n",
      "Spearman (Pseudo vs Real Accuracy): 0.23, Spearman (Real vs Real Accuracy): 0.47\n"
     ]
    }
   ],
   "source": [
    "# Image Method\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from utils.dataset import get_dataset\n",
    "from utils.model_utils import load_model, get_text_embeddings, get_image_embeddings\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "datasets_info = [\n",
    "    {\"name\": \"NWPU-RESISC45\", \"template\": \"a satellite photo containing {}.\"},\n",
    "    {\"name\": \"Stanford_dogs\", \"template\": \"a photo of {}, a type of dog.\"},\n",
    "    {\"name\": \"CUB_200_2011\", \"template\": \"a photo of {}, a type of bird.\"},\n",
    "    {\"name\": \"Flower102\", \"template\": \"a photo of {}, a type of flower.\"},\n",
    "]\n",
    "\n",
    "class PseudoImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, class_names, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Store class_names in a set for faster lookup\n",
    "        class_name_set = set(class_names)\n",
    "\n",
    "        for image_path in glob.glob(os.path.join(image_folder, '*.png')):\n",
    "            filename = os.path.basename(image_path)\n",
    "            class_name = filename.split('-')[0].replace('_', ' ')\n",
    "            if class_name in class_name_set:\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(class_name)\n",
    "\n",
    "        # Convert labels to indices\n",
    "        self.labels = [class_names.index(label) for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_image_loader(dataset, class_indices, batch_size, shuffle=False):\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        selected_indices = [i for i, label in enumerate(dataset.targets) if label in class_indices]\n",
    "    else:\n",
    "        selected_indices = [i for i, (_, label) in enumerate(dataset.imgs) if label in class_indices]\n",
    "    selected_dataset = Subset(dataset, selected_indices)\n",
    "\n",
    "    # Ensure no transform or compatible transform is applied\n",
    "    selected_dataset.dataset.transform = None\n",
    "\n",
    "    loader = DataLoader(\n",
    "        selected_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def pairwise_difference(a, b):\n",
    "  \"\"\"\n",
    "  Computes pairwise difference between rows of two matrices.\n",
    "\n",
    "  Args:\n",
    "    a: NumPy array of shape (N_a, D)\n",
    "    b: NumPy array of shape (N_b, D)\n",
    "\n",
    "  Returns:\n",
    "    NumPy array of shape (N_a, N_b, D) containing pairwise differences.\n",
    "  \"\"\"\n",
    "  return a[:, np.newaxis, :] - b[np.newaxis, :, :]\n",
    "\n",
    "def run_experiment(dataset_name, template, use_pseudo_data):\n",
    "    # 1. Load dataset (test split)\n",
    "    dataset = get_dataset(dataset_name, data_root='data', split='test')\n",
    "    class_names = dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    if use_pseudo_data:\n",
    "        PSEUDO_IMAGES_FOLDER = f'pseudo_images/{dataset_name}'\n",
    "        pseudo_dataset = PseudoImageDataset(\n",
    "            image_folder=PSEUDO_IMAGES_FOLDER,\n",
    "            class_names=class_names,\n",
    "            transform=None\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            pseudo_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "        )\n",
    "    else:\n",
    "        selected_class_indices = list(range(num_classes))\n",
    "        dataloader = get_image_loader(dataset, selected_class_indices, 64, shuffle=False)\n",
    "\n",
    "    # 2. Load model\n",
    "    model_info = load_model(model_name, device=device)\n",
    "\n",
    "    # 3. Create captions and compute text embeddings\n",
    "    captions = [template.format(c) for c in class_names]\n",
    "    text_embeddings = get_text_embeddings(captions, model_info, device, batch_size=64)\n",
    "\n",
    "    # 4. Compute image embeddings\n",
    "    image_embeddings, all_targets = get_image_embeddings(dataloader, model_info, device)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "    text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # img shape: (N_img, D), text shape: (N_text, D)\n",
    "\n",
    "    # 5. Compute similarities and predictions\n",
    "    temperature = 0.01\n",
    "    similarities = np.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "    probs = np.exp(similarities) / np.sum(np.exp(similarities), axis=1, keepdims=True)\n",
    "\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "\n",
    "    predicted_cm_scores = []\n",
    "    # do some class-wise operations\n",
    "    for c in range(num_classes):\n",
    "        # Select rows of image_embeddings where the target is class c\n",
    "        c_embeddings = image_embeddings[all_targets == c] # shape (N_c, D)\n",
    "\n",
    "        other_class_text_embeddings = np.concatenate([text_embeddings[i:i+1] for i in range(num_classes) if i != c], axis=0) # shape (N_t-1, D)\n",
    "        text_diff = pairwise_difference(text_embeddings[c:c+1], other_class_text_embeddings) # shape (1, N_t-1, D)\n",
    "        \n",
    "        c_embeddings = c_embeddings / np.linalg.norm(c_embeddings, axis=-1, keepdims=True)\n",
    "        text_diff = text_diff / np.linalg.norm(text_diff, axis=-1, keepdims=True)\n",
    "\n",
    "        cm_scores = np.einsum('cd, td -> ct', c_embeddings, text_diff[0]) # shape (N_c, N_t-1)\n",
    "        \n",
    "        # make softmax\n",
    "        def softmax(x):\n",
    "            \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "            e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        \n",
    "        def softmin(x):\n",
    "            \"\"\"Compute softmin values for each sets of scores in x (with numerical stability).\"\"\"\n",
    "            neg_x = -x\n",
    "            e_neg_x = np.exp(neg_x - np.max(neg_x, axis=-1, keepdims=True))\n",
    "            return e_neg_x / np.sum(e_neg_x, axis=-1, keepdims=True)\n",
    "        \n",
    "        cm_scores = softmin(cm_scores)\n",
    "\n",
    "        cm_score = np.min(cm_scores, axis=-1) # shape (N_c, 1)\n",
    "        # cm_score = cm_score / (1 + cm_score) # shape (N_c, 1)\n",
    "        \n",
    "        cm_score = np.mean(cm_score, axis=0) # shape (1,)\n",
    "        predicted_cm_scores.append(cm_score)\n",
    "\n",
    "    # Compute class-wise actual and predicted accuracies\n",
    "    actual_accuracies = []\n",
    "    # predicted_accuracies = []\n",
    "    for c in range(num_classes):\n",
    "        cur_class_indices = (all_targets == c)\n",
    "        cur_class_pred = preds[cur_class_indices]\n",
    "        cur_class_accuracy = (cur_class_pred == c).mean()\n",
    "\n",
    "        if np.isnan(cur_class_accuracy):\n",
    "            print(f\"Class {c}, {class_names[c]} has NaN accuracy\")\n",
    "            cur_class_accuracy = 0.0\n",
    "        # predicted_cur_class_indices = (preds == c)\n",
    "        # predicted_confidences = confidences[predicted_cur_class_indices]\n",
    "        # predicted_acc = predicted_confidences.mean() if predicted_confidences.size > 0 else np.nan\n",
    "\n",
    "        actual_accuracies.append(cur_class_accuracy)\n",
    "        # predicted_accuracies.append(predicted_acc if not np.isnan(predicted_acc) else 0.0)\n",
    "            \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr = spearmanr(np.array(actual_accuracies), predicted_cm_scores )# np.array(predicted_accuracies))\n",
    "\n",
    "    # Return data for plotting outside\n",
    "    return np.array(actual_accuracies), np.array(predicted_cm_scores), spearman_corr\n",
    "\n",
    "# Now we call run_experiment for pseudo and real data and plot them together\n",
    "for dinfo in datasets_info:\n",
    "    actual_pseudo, predicted_pseudo, spearman_pseudo = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=True)\n",
    "    actual_real, predicted_real, spearman_real = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=False)\n",
    "\n",
    "    # Recompute pseudo Spearman correlation using actual_real for the y-values\n",
    "    pseudo_spearman_corr = spearmanr(actual_real, predicted_pseudo)\n",
    "\n",
    "    # Create a single plot for both pseudo and real\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    # For pseudo, use actual_real as y-axis\n",
    "    ax.scatter(predicted_pseudo, actual_real, color='tab:orange', label='Pseudo Images', alpha=0.5)\n",
    "    ax.scatter(predicted_real, actual_real, color='tab:blue', label='Real Images', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Accuracy (Confidence)\")\n",
    "    ax.set_ylabel(\"Actual Accuracy (Real Data)\")\n",
    "    pseudo_corr_str = f\"{pseudo_spearman_corr.correlation:.2f}\" if pseudo_spearman_corr.correlation is not None else \"N/A\"\n",
    "    real_corr_str = f\"{spearman_real.correlation:.2f}\" if spearman_real.correlation is not None else \"N/A\"\n",
    "    ax.set_title(f\"Calibration Approach: {dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    print(f\"Calibration Approach: {dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    ax.grid(True)\n",
    "    # ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plot_filename = f\"figures/{dinfo['name']}_CM_softmin.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 98/98 [04:33<00:00,  2.79s/it]\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 93, White breasted Nuthatch has NaN cm_score\n",
      "Class 93, White breasted Nuthatch has NaN accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wuy19\\AppData\\Local\\Temp\\ipykernel_100040\\209729268.py:189: RuntimeWarning: Mean of empty slice.\n",
      "  cur_class_accuracy = (cur_class_pred == c).mean()\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/4\n",
      "Processed batch 2/4\n",
      "Processed batch 3/4\n",
      "Processed batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 185/185 [02:26<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUB_200_2011\n",
      "Spearman (Pseudo vs Real Accuracy): 0.73, Spearman (Real vs Real Accuracy): 1.00\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 51/51 [00:49<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 11, black-eyed susan has NaN cm_score\n",
      "Class 26, colt's foot has NaN cm_score\n",
      "Class 32, desert-rose has NaN cm_score\n",
      "Class 44, globe-flower has NaN cm_score\n",
      "Class 47, hard-leaved pocket orchid has NaN cm_score\n",
      "Class 72, pink-yellow dahlia has NaN cm_score\n",
      "Class 79, ruby-lipped cattleya has NaN cm_score\n",
      "Class 11, black-eyed susan has NaN accuracy\n",
      "Class 26, colt's foot has NaN accuracy\n",
      "Class 32, desert-rose has NaN accuracy\n",
      "Class 44, globe-flower has NaN accuracy\n",
      "Class 47, hard-leaved pocket orchid has NaN accuracy\n",
      "Class 72, pink-yellow dahlia has NaN accuracy\n",
      "Class 79, ruby-lipped cattleya has NaN accuracy\n",
      "Processed batch 1/2\n",
      "Processed batch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 128/128 [01:16<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flower102\n",
      "Spearman (Pseudo vs Real Accuracy): 0.48, Spearman (Real vs Real Accuracy): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Image Method\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from utils.dataset import get_dataset\n",
    "from utils.model_utils import load_model, get_text_embeddings, get_image_embeddings\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "datasets_info = [\n",
    "    # {\"name\": \"NWPU-RESISC45\", \"template\": \"a satellite photo containing {}.\"},\n",
    "    # {\"name\": \"Stanford_dogs\", \"template\": \"a photo of {}, a type of dog.\"},\n",
    "    {\"name\": \"CUB_200_2011\", \"template\": \"a photo of {}, a type of bird.\"},\n",
    "    {\"name\": \"Flower102\", \"template\": \"a photo of {}, a type of flower.\"},\n",
    "]\n",
    "\n",
    "class PseudoImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, class_names, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Store class_names in a set for faster lookup\n",
    "        class_name_set = set(class_names)\n",
    "\n",
    "        for image_path in glob.glob(os.path.join(image_folder, '*.png')):\n",
    "            filename = os.path.basename(image_path)\n",
    "            class_name = filename.split('-')[0].replace('_', ' ')\n",
    "            if class_name in class_name_set:\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(class_name)\n",
    "\n",
    "        # Convert labels to indices\n",
    "        self.labels = [class_names.index(label) for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_image_loader(dataset, class_indices, batch_size, shuffle=False):\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        selected_indices = [i for i, label in enumerate(dataset.targets) if label in class_indices]\n",
    "    else:\n",
    "        selected_indices = [i for i, (_, label) in enumerate(dataset.imgs) if label in class_indices]\n",
    "    selected_dataset = Subset(dataset, selected_indices)\n",
    "\n",
    "    # Ensure no transform or compatible transform is applied\n",
    "    selected_dataset.dataset.transform = None\n",
    "\n",
    "    loader = DataLoader(\n",
    "        selected_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def pairwise_difference(a, b):\n",
    "  \"\"\"\n",
    "  Computes pairwise difference between rows of two matrices.\n",
    "\n",
    "  Args:\n",
    "    a: NumPy array of shape (N_a, D)\n",
    "    b: NumPy array of shape (N_b, D)\n",
    "\n",
    "  Returns:\n",
    "    NumPy array of shape (N_a, N_b, D) containing pairwise differences.\n",
    "  \"\"\"\n",
    "  return a[:, np.newaxis, :] - b[np.newaxis, :, :]\n",
    "\n",
    "def run_experiment(dataset_name, template, use_pseudo_data):\n",
    "    # 1. Load dataset (test split)\n",
    "    dataset = get_dataset(dataset_name, data_root='data', split='test')\n",
    "    class_names = dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    if use_pseudo_data:\n",
    "        PSEUDO_IMAGES_FOLDER = f'pseudo_images/{dataset_name}'\n",
    "        pseudo_dataset = PseudoImageDataset(\n",
    "            image_folder=PSEUDO_IMAGES_FOLDER,\n",
    "            class_names=class_names,\n",
    "            transform=None\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            pseudo_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda batch: (list(zip(*batch))[0], list(zip(*batch))[1])\n",
    "        )\n",
    "    else:\n",
    "        selected_class_indices = list(range(num_classes))\n",
    "        dataloader = get_image_loader(dataset, selected_class_indices, 64, shuffle=False)\n",
    "\n",
    "    # 2. Load model\n",
    "    model_info = load_model(model_name, device=device)\n",
    "\n",
    "    # 3. Create captions and compute text embeddings\n",
    "    captions = [template.format(c) for c in class_names]\n",
    "    text_embeddings = get_text_embeddings(captions, model_info, device, batch_size=64)\n",
    "\n",
    "    # 4. Compute image embeddings\n",
    "    image_embeddings, all_targets = get_image_embeddings(dataloader, model_info, device)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "    text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # img shape: (N_img, D), text shape: (N_text, D)\n",
    "\n",
    "    # 5. Compute similarities and predictions\n",
    "    temperature = 0.01\n",
    "    similarities = np.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "    probs = np.exp(similarities) / np.sum(np.exp(similarities), axis=1, keepdims=True)\n",
    "\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "\n",
    "    predicted_cm_scores = []\n",
    "    # do some class-wise operations\n",
    "    for c in range(num_classes):\n",
    "        # Select rows of image_embeddings where the target is class c\n",
    "        c_embeddings = image_embeddings[all_targets == c] # shape (N_c, D)\n",
    "\n",
    "        other_class_text_embeddings = np.concatenate([text_embeddings[i:i+1] for i in range(num_classes) if i != c], axis=0) # shape (N_t-1, D)\n",
    "        text_diff = pairwise_difference(text_embeddings[c:c+1], other_class_text_embeddings) # shape (1, N_t-1, D)\n",
    "        \n",
    "        c_embeddings = c_embeddings / np.linalg.norm(c_embeddings, axis=-1, keepdims=True)\n",
    "        text_diff = text_diff / np.linalg.norm(text_diff, axis=-1, keepdims=True)\n",
    "\n",
    "        cm_scores = np.einsum('cd, td -> ct', c_embeddings, text_diff[0]) # shape (N_c, N_t-1)\n",
    "        \n",
    "        # make softmax\n",
    "        def softmax(x):\n",
    "            \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "            e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "        \n",
    "        def softmin(x):\n",
    "            \"\"\"Compute softmin values for each sets of scores in x (with numerical stability).\"\"\"\n",
    "            neg_x = -x\n",
    "            e_neg_x = np.exp(neg_x - np.max(neg_x, axis=1, keepdims=True))\n",
    "            return e_neg_x / np.sum(e_neg_x, axis=1, keepdims=True)\n",
    "        \n",
    "        # cm_scores = softmax(1/cm_scores)\n",
    "\n",
    "        cm_score = np.min(cm_scores, axis=-1) # shape (N_c, 1)\n",
    "        # # cm_score = cm_score / (1 + cm_score) # shape (N_c, 1)\n",
    "        \n",
    "        def mask_score(cm_score):\n",
    "            positive_or_zero_mask = cm_score >= 0\n",
    "            negative_mask = cm_score < 0\n",
    "            cm_score[positive_or_zero_mask] = 1\n",
    "            cm_score[negative_mask] = 0\n",
    "            return cm_score\n",
    "\n",
    "        cm_score = mask_score(cm_score)\n",
    "        cm_score = np.mean(cm_score, axis=0) # shape (1,)\n",
    "\n",
    "        if np.isnan(cm_score):\n",
    "            print(f\"Class {c}, {class_names[c]} has NaN cm_score\")\n",
    "            cm_score = 0.0\n",
    "\n",
    "        predicted_cm_scores.append(cm_score)\n",
    "\n",
    "    # Compute class-wise actual and predicted accuracies\n",
    "    actual_accuracies = []\n",
    "    # predicted_accuracies = []\n",
    "    for c in range(num_classes):\n",
    "        cur_class_indices = (all_targets == c)\n",
    "        cur_class_pred = preds[cur_class_indices]\n",
    "        cur_class_accuracy = (cur_class_pred == c).mean()\n",
    "\n",
    "        if np.isnan(cur_class_accuracy):\n",
    "            print(f\"Class {c}, {class_names[c]} has NaN accuracy\")\n",
    "            cur_class_accuracy = 0.0\n",
    "        # predicted_cur_class_indices = (preds == c)\n",
    "        # predicted_confidences = confidences[predicted_cur_class_indices]\n",
    "        # predicted_acc = predicted_confidences.mean() if predicted_confidences.size > 0 else np.nan\n",
    "\n",
    "        actual_accuracies.append(cur_class_accuracy)\n",
    "        # predicted_accuracies.append(predicted_acc if not np.isnan(predicted_acc) else 0.0)\n",
    "            \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr = spearmanr(np.array(actual_accuracies), predicted_cm_scores )# np.array(predicted_accuracies))\n",
    "\n",
    "    # Return data for plotting outside\n",
    "    return np.array(actual_accuracies), np.array(predicted_cm_scores), spearman_corr\n",
    "\n",
    "# Now we call run_experiment for pseudo and real data and plot them together\n",
    "for dinfo in datasets_info:\n",
    "    actual_pseudo, predicted_pseudo, spearman_pseudo = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=True)\n",
    "    actual_real, predicted_real, spearman_real = run_experiment(dinfo[\"name\"], dinfo[\"template\"], use_pseudo_data=False)\n",
    "\n",
    "    # Recompute pseudo Spearman correlation using actual_real for the y-values\n",
    "    pseudo_spearman_corr = spearmanr(actual_real, predicted_pseudo)\n",
    "\n",
    "    # Create a single plot for both pseudo and real\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    # For pseudo, use actual_real as y-axis\n",
    "    ax.scatter(predicted_pseudo, actual_real, color='tab:orange', label='Pseudo Images', alpha=0.5)\n",
    "    ax.scatter(predicted_real, actual_real, color='tab:blue', label='Real Images', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Accuracy (Confidence)\")\n",
    "    ax.set_ylabel(\"Actual Accuracy (Real Data)\")\n",
    "    pseudo_corr_str = f\"{pseudo_spearman_corr.correlation:.2f}\" if pseudo_spearman_corr.correlation is not None else \"N/A\"\n",
    "    real_corr_str = f\"{spearman_real.correlation:.2f}\" if spearman_real.correlation is not None else \"N/A\"\n",
    "    ax.set_title(f\"{dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    print(f\"{dinfo['name']}\\nSpearman (Pseudo vs Real Accuracy): {pseudo_corr_str}, Spearman (Real vs Real Accuracy): {real_corr_str}\")\n",
    "    ax.grid(True)\n",
    "    # ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plot_filename = f\"figures/{dinfo['name']}_CM_count.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------------------+--------------------------------------------+-------------------------------------------------+------------------------------------------+\n",
      "|    Dataset    | Spearman (Pseudo vs Real) - Discretization Method | Spearman (Pseudo vs Real) - Default method | Spearman (Real vs Real) - Discretization Method | Spearman (Real vs Real) - Default method |\n",
      "+---------------+---------------------------------------------------+--------------------------------------------+-------------------------------------------------+------------------------------------------+\n",
      "| Stanford_dogs |                        0.59                       |                  **0.62**                  |                     **1.00**                    |                   0.98                   |\n",
      "| NWPU-RESISC45 |                      **0.61**                     |                    0.58                    |                     **1.00**                    |                   0.98                   |\n",
      "|   Flower102   |                      **0.48**                     |                    0.35                    |                     **1.00**                    |                   0.96                   |\n",
      "|  CUB_200_2011 |                        0.73                       |                  **0.76**                  |                     **1.00**                    |                   0.99                   |\n",
      "+---------------+---------------------------------------------------+--------------------------------------------+-------------------------------------------------+------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Dataset\", \"Spearman (Pseudo vs Real) - Discretization Method\", \"Spearman (Pseudo vs Real) - Default method\", \"Spearman (Real vs Real) - Discretization Method\", \"Spearman (Real vs Real) - Default method\"]\n",
    "table.add_row([\"Stanford_dogs\", \"0.59\", \"**0.62**\", \"**1.00**\", \"0.98\"])\n",
    "table.add_row([\"NWPU-RESISC45\", \"**0.61**\", \"0.58\", \"**1.00**\", \"0.98\"])\n",
    "table.add_row([\"Flower102\", \"**0.48**\", \"0.35\", \"**1.00**\", \"0.96\"])\n",
    "table.add_row([\"CUB_200_2011\", \"0.73\", \"**0.76**\", \"**1.00**\", \"0.99\"])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
