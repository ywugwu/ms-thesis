{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded successfully from 'features_at_layer.npz'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the .npz file\n",
    "loaded = np.load('features_at_layer.npz')\n",
    "\n",
    "# Reconstruct the features_at_layer dictionary\n",
    "features_at_layer = {}\n",
    "for key in loaded:\n",
    "    layer, feature_type = key.rsplit('_', 1)\n",
    "    if layer not in features_at_layer:\n",
    "        features_at_layer[layer] = {}\n",
    "    features_at_layer[layer][feature_type] = loaded[key]\n",
    "# feature type: 'attn', 'mlp'\n",
    "print(\"Features loaded successfully from 'features_at_layer.npz'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthfulqa_predictions = pd.read_csv(\"truthfulqa_predictions.csv\")\n",
    "truthfulqa_no_predictions = truthfulqa_predictions[truthfulqa_predictions.extracted_answer == -1]\n",
    "truthfulqa_predictions = truthfulqa_predictions[truthfulqa_predictions.extracted_answer != -1]\n",
    "\n",
    "# only load the incorrect sampels\n",
    "truthfulqa_incorrect_predictions = truthfulqa_predictions[truthfulqa_predictions.reference != truthfulqa_predictions.extracted_answer].reset_index(drop=True)\n",
    "truthfulqa_correct_predictions = truthfulqa_predictions[truthfulqa_predictions.reference == truthfulqa_predictions.extracted_answer].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Limit to 32 samples each for speed\n",
    "n_samples = 100\n",
    "incorrect_samples = truthfulqa_incorrect_predictions.sample(n=n_samples, random_state=0)\n",
    "correct_samples = truthfulqa_correct_predictions.sample(n=n_samples, random_state=0)\n",
    "no_samples = truthfulqa_no_predictions.sample(n=n_samples, random_state=0)\n",
    "# Combine into one DataFrame\n",
    "combined_samples = pd.concat([incorrect_samples, correct_samples, no_samples], ignore_index=True)\n",
    "combined_samples['label'] = ['Incorrect'] * n_samples + ['Correct'] * n_samples + ['No Answer'] * n_samples\n",
    "\n",
    "chat_template = \"<|user|>\\n{instruction}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "combined_samples['formatted_prompt'] = combined_samples['prompt'].apply(\n",
    "    lambda x: chat_template.format(instruction=x)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming features_at_layer is a dictionary: {layer_idx: features_list}\n",
    "mean_features_per_label = {}  # {layer_idx: {label: {'attn_features': np.array, 'mlp_features': np.array}}}\n",
    "\n",
    "labels = combined_samples['label'].values  # Array of labels corresponding to the features\n",
    "\n",
    "for layer_idx, features_list in features_at_layer.items():\n",
    "    # Initialize a dictionary to collect features per label\n",
    "    attn_features_per_label = {'Correct': [], 'Incorrect': [], 'No Answer': []}\n",
    "    mlp_features_per_label = {'Correct': [], 'Incorrect': [], 'No Answer': []}\n",
    "    \n",
    "    # for feature_type in features_list: # ['attn', 'mlp']\n",
    "    for i, feature_dict in enumerate(features_list['attn']):\n",
    "        label = labels[i]\n",
    "        attn_features_per_label[label].append(feature_dict)\n",
    "    for i, feature_dict in enumerate(features_list['mlp']):\n",
    "        label = labels[i]\n",
    "        mlp_features_per_label[label].append(feature_dict)\n",
    "        \n",
    "    mean_features = {}\n",
    "    for label in attn_features_per_label:\n",
    "        mean_features[label] = {\n",
    "            'attn_features': np.mean(attn_features_per_label[label], axis=0),  # Average across samples\n",
    "            'mlp_features': np.mean(mlp_features_per_label[label], axis=0)  # Average across samples\n",
    "        }\n",
    "    mean_features_per_label[layer_idx] = mean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_vectors_per_layer = {}  # {layer_idx: {(label_A, label_B): {'attn_features': np.array, 'mlp_features': np.array}}}\n",
    "\n",
    "label_pairs = [('Incorrect', 'Correct'), ('Incorrect', 'No Answer'), ('Correct', 'No Answer')] + \\\n",
    "                [('Correct', 'Incorrect'), ('No Answer', 'Incorrect'), ('No Answer', 'Correct')] \n",
    "\n",
    "for layer_idx, mean_features in mean_features_per_label.items():\n",
    "    direction_vectors = {}\n",
    "    for (label_A, label_B) in label_pairs:\n",
    "        # Ensure both labels have mean features computed\n",
    "        if label_A in mean_features and label_B in mean_features:\n",
    "            dir_vector_attn = mean_features[label_B]['attn_features'] - mean_features[label_A]['attn_features']\n",
    "            dir_vector_mlp = mean_features[label_B]['mlp_features'] - mean_features[label_A]['mlp_features']\n",
    "            direction_vectors[(label_A, label_B)] = {\n",
    "                'attn_features': dir_vector_attn,\n",
    "                'mlp_features': dir_vector_mlp\n",
    "            }\n",
    "            # dir = v_b - v_a\n",
    "            # so v_a + dir = v_b\n",
    "    direction_vectors_per_layer[layer_idx] = direction_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_with_activation_modification(\n",
    "    model, tokenizer, texts, layer_idxs, module_type, mlp_direction_vectors=None, attn_direction_vectors=None, device='cuda', max_new_tokens=50\n",
    "):\n",
    "    model.eval()\n",
    "    generated_sequences = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    for layer_idx in layer_idxs:\n",
    "        layer = model.model.layers[layer_idx]\n",
    "\n",
    "        if mlp_direction_vectors is not None and layer_idx in mlp_direction_vectors:\n",
    "            dir_vector = mlp_direction_vectors[layer_idx]\n",
    "            if layer_idx == layer_idxs[0]:\n",
    "                print(f\"Apply MLP dir_vector at layer {layer_idx} with shape: {dir_vector.shape}\")\n",
    "\n",
    "            def create_hook_fn_mlp(dir_vector):\n",
    "                def hook_fn_mlp(module, input, output):\n",
    "                    # output should be a tensor\n",
    "                    output_tensor = output\n",
    "                    modified_output = output_tensor + dir_vector.unsqueeze(0).unsqueeze(0)\n",
    "                    return modified_output\n",
    "                return hook_fn_mlp\n",
    "\n",
    "            hook_fn_mlp = create_hook_fn_mlp(dir_vector)\n",
    "            if module_type == 'mlp' or module_type == 'both':\n",
    "                handle = layer.mlp.register_forward_hook(hook_fn_mlp)\n",
    "                handles.append(handle)\n",
    "\n",
    "        if attn_direction_vectors is not None and layer_idx in attn_direction_vectors:\n",
    "            dir_vector = attn_direction_vectors[layer_idx]\n",
    "            if layer_idx == layer_idxs[0]:\n",
    "                print(f\"Apply Attention dir_vector at layer {layer_idx} with shape: {dir_vector.shape}\")\n",
    "\n",
    "            def create_hook_fn_attn(dir_vector):\n",
    "                def hook_fn_attn(module, input, output):\n",
    "                    # output may be a tuple\n",
    "                    if isinstance(output, tuple):\n",
    "                        output_tensor = output[0]\n",
    "                        modified_output = output_tensor + dir_vector.unsqueeze(0).unsqueeze(0)\n",
    "                        # Return modified output in place of output[0]\n",
    "                        return (modified_output,) + output[1:]\n",
    "                    else:\n",
    "                        output_tensor = output\n",
    "                        modified_output = output_tensor + dir_vector.unsqueeze(0).unsqueeze(0)\n",
    "                        return modified_output\n",
    "                return hook_fn_attn\n",
    "\n",
    "            hook_fn_attn = create_hook_fn_attn(dir_vector)\n",
    "            if module_type == 'attn' or module_type == 'both':\n",
    "                handle = layer.self_attn.register_forward_hook(hook_fn_attn)\n",
    "                handles.append(handle)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "\n",
    "            # Generate sequences\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the generated tokens to text\n",
    "            generated_text = tokenizer.decode(\n",
    "                output_ids[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            generated_sequences.append(generated_text)\n",
    "\n",
    "    # Remove the hooks if they were registered\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    del handles\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Incorrect', 'Correct', 'No Answer'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_samples.label.unique() # array(['Incorrect', 'Correct', 'No Answer'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc351147769421eac7db07c208418b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Adjust based on your hardware\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline responses without any modifications\n",
    "# labels = ['Incorrect', 'Correct', 'No Answer']\n",
    "\n",
    "# correct_samples = combined_samples[combined_samples['label'] == 'Correct']\n",
    "# texts = correct_samples['formatted_prompt'].tolist()[:10]\n",
    "\n",
    "# baseline_responses = generate_responses_with_activation_modification(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     texts,\n",
    "#     layer_idxs=[],  # No layers to modify\n",
    "#     module_type=None,\n",
    "#     mlp_direction_vectors=None,\n",
    "#     attn_direction_vectors=None,\n",
    "#     device='cuda',\n",
    "#     max_new_tokens=50\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"\n",
    "    Extracts the answer from the model's response using a regular expression.\n",
    "    Adjust the regex pattern based on the expected format of the response.\n",
    "    \"\"\"\n",
    "    # Example regex patterns (adjust these based on your data)\n",
    "    # Pattern to match \"Answer: Yes/No\"\n",
    "    match = re.search(r\"correct choice is ([0-9])\", response)\n",
    "    if match:\n",
    "        return match.group(1).capitalize()\n",
    "    return None\n",
    "# Function to determine if an answer is 'Correct' or 'Incorrect'\n",
    "def determine_label(answer, response):\n",
    "    # Implement logic to map answers to labels\n",
    "    # For example, if 'Yes' is considered 'Correct'\n",
    "    # consider 'response' after <|assistant|>\n",
    "    response_after = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    if 'Yes' not in response_after and 'Sorry' not in response_after:\n",
    "        return 'Miss'\n",
    "    if answer is None:\n",
    "        return 'No Answer'\n",
    "    if answer.isdigit():\n",
    "        if answer == '1':\n",
    "            return 'Correct'\n",
    "        else:\n",
    "            return 'Incorrect'\n",
    "    else:\n",
    "        return 'No Answer'\n",
    "# Example string output from ChatGPT\n",
    "output = \"\"\"\n",
    "Yes, I know and the correct choice is 1.\n",
    "*Reasons*: Nauru is indeed the world's smallest independent republic, with an area of\n",
    "\"\"\"\n",
    "print(extract_answer(output))\n",
    "print(determine_label(extract_answer(output),output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\Wuy19\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wuy19\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wuy19\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 100/100 [03:39<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated baseline responses for label 'Incorrect'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:43<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated baseline responses for label 'Correct'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:25<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated baseline responses for label 'No Answer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate baseline responses for all labels\n",
    "labels = ['Incorrect', 'Correct', 'No Answer']\n",
    "n_samples_per_label = 100  # Adjust as needed\n",
    "\n",
    "baseline_responses = {}\n",
    "baseline_labels = {}\n",
    "\n",
    "for label in labels:\n",
    "    samples = combined_samples[combined_samples['label'] == label].sample(n=n_samples_per_label, random_state=42)\n",
    "    prompts = samples['formatted_prompt'].tolist()\n",
    "    responses = generate_responses_with_activation_modification(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        texts=prompts,\n",
    "        layer_idxs=[],  # No modifications\n",
    "        module_type=None,\n",
    "        mlp_direction_vectors=None,\n",
    "        attn_direction_vectors=None,\n",
    "        device='cuda',\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    extracted_answers = [extract_answer(resp) for resp in responses]\n",
    "    mapped_labels = [determine_label(answer, resp) for answer, resp in zip(extracted_answers, responses)]\n",
    "    baseline_responses[label] = responses\n",
    "    baseline_labels[label] = mapped_labels\n",
    "    print(f\"Generated baseline responses for label '{label}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying direction vector from 'Incorrect' to 'Correct'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('Incorrect', 'Correct').\n",
      "\n",
      "Applying direction vector from 'Incorrect' to 'No Answer'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('Incorrect', 'No Answer').\n",
      "\n",
      "Applying direction vector from 'Correct' to 'Incorrect'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('Correct', 'Incorrect').\n",
      "\n",
      "Applying direction vector from 'Correct' to 'No Answer'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('Correct', 'No Answer').\n",
      "\n",
      "Applying direction vector from 'No Answer' to 'Incorrect'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('No Answer', 'Incorrect').\n",
      "\n",
      "Applying direction vector from 'No Answer' to 'Correct'...\n",
      "Apply MLP dir_vector at layer 16 with shape: torch.Size([3072])\n",
      "Apply Attention dir_vector at layer 16 with shape: torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated modified responses for pair ('No Answer', 'Correct').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Define all possible label pairs excluding same labels\n",
    "label_pairs = [\n",
    "    ('Incorrect', 'Correct'),\n",
    "    ('Incorrect', 'No Answer'),\n",
    "    ('Correct', 'Incorrect'),\n",
    "    ('Correct', 'No Answer'),\n",
    "    ('No Answer', 'Incorrect'),\n",
    "    ('No Answer', 'Correct')\n",
    "]\n",
    "\n",
    "# Initialize dictionaries to store modified responses and labels\n",
    "modified_responses = {pair: [] for pair in label_pairs}\n",
    "modified_labels = {pair: [] for pair in label_pairs}\n",
    "\n",
    "# Specify the layers you want to modify\n",
    "layer_idxs = [16, 17, 18, 19, 20, 21, 22, ]\n",
    "\n",
    "# Iterate over each label pair and generate modified responses\n",
    "for pair in label_pairs:\n",
    "    label_A, label_B = pair\n",
    "    print(f\"\\nApplying direction vector from '{label_A}' to '{label_B}'...\")\n",
    "    \n",
    "    # Prepare direction vectors\n",
    "    mlp_direction_vectors = {}\n",
    "    attn_direction_vectors = {}\n",
    "    \n",
    "    for layer_idx in layer_idxs:\n",
    "        layer_idx_str = str(layer_idx)\n",
    "        try:\n",
    "            mlp_vector = direction_vectors_per_layer[layer_idx_str][pair]['mlp_features']\n",
    "            attn_vector = direction_vectors_per_layer[layer_idx_str][pair]['attn_features']\n",
    "            mlp_direction_vectors[layer_idx] = torch.from_numpy(mlp_vector).cuda().to(torch.bfloat16)\n",
    "            attn_direction_vectors[layer_idx] = torch.from_numpy(attn_vector).cuda().to(torch.bfloat16)\n",
    "        except KeyError:\n",
    "            print(f\"Direction vectors for pair {pair} not found in layer {layer_idx}. Skipping this layer.\")\n",
    "            continue\n",
    "    \n",
    "    # Select samples with label_A\n",
    "    samples = combined_samples[combined_samples['label'] == label_A].sample(n=n_samples_per_label, random_state=42)\n",
    "    prompts = samples['formatted_prompt'].tolist()\n",
    "    \n",
    "    # Generate modified responses\n",
    "    responses = generate_responses_with_activation_modification(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        texts=prompts,\n",
    "        layer_idxs=layer_idxs,\n",
    "        module_type='both',\n",
    "        mlp_direction_vectors=mlp_direction_vectors,\n",
    "        attn_direction_vectors=attn_direction_vectors,\n",
    "        device='cuda',\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    \n",
    "    # Extract and map labels\n",
    "    extracted_answers = [extract_answer(resp) for resp in responses]\n",
    "    mapped_labels = [determine_label(answer, resp) for answer, resp in zip(extracted_answers, responses)]\n",
    "    \n",
    "    # Store the results\n",
    "    modified_responses[pair] = responses\n",
    "    modified_labels[pair] = mapped_labels\n",
    "    print(f\"Generated modified responses for pair {pair}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Miss', 'No Answer'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(mapped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 'Incorrect' to 'Correct':\n",
      "    Correct: 2 out of 100 samples.\n",
      "    No change: 77 out of 100 samples.\n",
      "    No Answer: 20 out of 100 samples.\n",
      "    Invalid: 1 out of 100 samples.\n",
      "    Success Rate: 0.02\n",
      "From 'Incorrect' to 'No Answer':\n",
      "    No Answer: 26 out of 100 samples.\n",
      "    No change: 67 out of 100 samples.\n",
      "    Correct: 5 out of 100 samples.\n",
      "    Invalid: 2 out of 100 samples.\n",
      "    Success Rate: 0.27\n",
      "From 'Correct' to 'Incorrect':\n",
      "    Incorrect: 6 out of 100 samples.\n",
      "    No change: 72 out of 100 samples.\n",
      "    No Answer: 22 out of 100 samples.\n",
      "    Invalid: 0 out of 100 samples.\n",
      "    Success Rate: 0.06\n",
      "From 'Correct' to 'No Answer':\n",
      "    No Answer: 7 out of 100 samples.\n",
      "    No change: 90 out of 100 samples.\n",
      "    Incorrect: 3 out of 100 samples.\n",
      "    Invalid: 0 out of 100 samples.\n",
      "    Success Rate: 0.07\n",
      "From 'No Answer' to 'Incorrect':\n",
      "    Incorrect: 0 out of 100 samples.\n",
      "    No change: 97 out of 100 samples.\n",
      "    Correct: 0 out of 100 samples.\n",
      "    Invalid: 3 out of 100 samples.\n",
      "    Success Rate: 0.00\n",
      "From 'No Answer' to 'Correct':\n",
      "    Correct: 0 out of 100 samples.\n",
      "    No change: 98 out of 100 samples.\n",
      "    Incorrect: 0 out of 100 samples.\n",
      "    Invalid: 2 out of 100 samples.\n",
      "    Success Rate: 0.00\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Initialize a dictionary to store shift rates\n",
    "shift_rates = {}\n",
    "\n",
    "# Compute shift rates for each label pair\n",
    "\n",
    "for label_A in baseline_labels:\n",
    "    baseline_predictions = baseline_labels[label_A]\n",
    "    for pair in label_pairs:\n",
    "        if pair[0] != label_A:\n",
    "            continue\n",
    "        label_B = pair[1]\n",
    "        modified_predictions = modified_labels[pair]\n",
    "        no_change = 0\n",
    "        invalid = 0\n",
    "        shift = 0\n",
    "        other_shift = 0\n",
    "        for i in range(len(modified_predictions)):\n",
    "            if modified_predictions[i] == 'Miss':\n",
    "                invalid += 1\n",
    "            elif modified_predictions[i] == label_A:\n",
    "                no_change += 1\n",
    "            elif modified_predictions[i] == label_B:\n",
    "                shift += 1\n",
    "            else:\n",
    "                other_shift += 1\n",
    "        shift_rates[pair] = {\n",
    "            'No Change': no_change,\n",
    "            'Shift': shift,\n",
    "            'Other Shift': other_shift,\n",
    "            'Invalid': invalid,\n",
    "            \"Success Rate\": shift / (shift + no_change + other_shift),\n",
    "        }\n",
    "        for other_shift_name in baseline_labels:\n",
    "            if other_shift_name != label_A and other_shift_name != label_B:\n",
    "                break\n",
    "        print(f\"From '{label_A}' to '{label_B}':\")\n",
    "        print(f\"    {label_B}: {shift} out of {n_samples_per_label} samples.\")\n",
    "        print(f\"    No change: {no_change} out of {n_samples_per_label} samples.\")\n",
    "        print(f\"    {other_shift_name}: {other_shift} out of {n_samples_per_label} samples.\")\n",
    "        print(f\"    Invalid: {invalid} out of {n_samples_per_label} samples.\")\n",
    "        print(f\"    Success Rate: {shift_rates[pair]['Success Rate']:.2f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+---------+--------------+\n",
      "| Success Rate From \\ To | Incorrect | Correct | I don't know |\n",
      "+------------------------+-----------+---------+--------------+\n",
      "|       Incorrect        |    NaN    |    2%   |     27%      |\n",
      "|        Correct         |     6%    |   NaN   |      7%      |\n",
      "|      I don't know      |     0%    |    0%   |     NaN      |\n",
      "+------------------------+-----------+---------+--------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\Wuy19\\AppData\\Local\\Temp\\ipykernel_50200\\2998939046.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  shift_table.field_names = [\"Success Rate From \\ To\"] + table_labels\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "# Define labels for the table\n",
    "table_labels = ['Incorrect', 'Correct', 'I don\\'t know']\n",
    "\n",
    "# Initialize a PrettyTable\n",
    "shift_table = PrettyTable()\n",
    "shift_table.field_names = [\"Success Rate From \\ To\"] + table_labels\n",
    "\n",
    "# Populate the table\n",
    "for from_label in table_labels:\n",
    "    row = [from_label]\n",
    "    if from_label == 'I don\\'t know':\n",
    "        from_label = 'No Answer'\n",
    "    \n",
    "    for to_label in table_labels:\n",
    "        if to_label == 'I don\\'t know':\n",
    "            to_label = 'No Answer'\n",
    "        if from_label == to_label:\n",
    "            row.append(\"NaN\")\n",
    "        else:\n",
    "            pair = (from_label, to_label)\n",
    "            if pair in shift_rates:\n",
    "                success_rate = shift_rates[pair][\"Success Rate\"]\n",
    "                row.append(f\"{success_rate*100:.0f}%\")\n",
    "            else:\n",
    "                row.append(\"NaN\")\n",
    "    shift_table.add_row(row)\n",
    "\n",
    "print(shift_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "----\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "----\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "naive_cnt = 0\n",
    "for response in modified_responses[('Incorrect', 'No Answer')]:\n",
    "    # print ans start after <|assistant|>\n",
    "    \n",
    "    if 'Yes' not in response.split(\"<|assistant|>\")[1].strip():\n",
    "        naive_cnt += 1\n",
    "        print(response.split(\"<|assistant|>\")[1].strip())\n",
    "        print(\"----\")\n",
    "    # else:\n",
    "        # print(response.split(\"<|assistant|>\")[1].strip())\n",
    "        # print(\"----\")\n",
    "print(naive_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-27): 28 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaSdpaAttention(\n",
       "      (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
